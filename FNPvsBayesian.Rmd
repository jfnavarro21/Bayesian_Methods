---
title: "BA Workshop2"
author: "John Navarro"
date: "November 1, 2017"
output: pdf_document
---
# 2. Review of MCMC binomial model

Code MCMC algorithm for simplest binomial model with conjugate prior
Model description contains 2 parts:
yi???Binom(p=??,size=1),
?????Beta(Aprior,Bprior),
where ?? is the unknown probability of success with prior beta distribution with fixed parameters A, B

## 2.1 Data

Create vector of zeros and ones of length k=20 and prob of success theta = 0.6
```{r}
set.seed(9374)
(Y<-rbinom(20,size=1,p=.6))
```

## 2.2 Running MCMC
Recall that Metropolis-Hastings MCMC algorithm consists of the following steps

### 2.2.1 Generate new proposal using some convenient distribution

Typically new proposals are generated from Gaussian distribution centered at the current value of Markov chain with sufficiently small variance.
Write a function for new proposals proposalFun<-function(oldTheta) which simulates newTheta from Gaussian distribution with mean value oldTheta and fixed standard deviation of small enough level to generate reasonable proposals for ???? between 0 and 1.
It is recommended to truncate the proposed values between extreme levels of, for example, 0.01 and 0.99 to prevent values exactly at 0 and 1.
```{r}
# Create a function that gives a new theta based on gaussian distribution
proposalFun<-function(oldTheta){
  newProp<-rnorm(1,oldTheta,tuneSigma)
  newprop<-min(newProp,.99)
  newProp<-max(newProp,.01)
  newProp
}
```
### 2.2.2 Decide whether the proposal newTheta is accepted or rejected.

To do that follow the logic:

If posterior probability density at newTheta is greater than posterior probability density at oldTheta then accept newTheta into the chain sample.
If posterior probability density at newTheta is smaller than posterior probability density at oldTheta then accept newTheta with probability
pdec=p(??new|y)p(??old|y)=p(y|??new)p(??new|A,B)p(y|??old)p(??old|A,B)
and reject newTheta with probability 1???pdec1???pdec.

Acceptance decision is made by simulating uniform random variable on [0,1] UU and comparing it with pdecpdec. If U<pdecU<pdec then  newTheta is accepted.

Write function binomialLikelihood<-function(Theta,Size,mySample) that calculates binomial likelihood p(??|y)p of sample mySample with binomial parameters Theta and size Size.

Also write function binomialPrior<-function(Theta,A,B) that calculates prior distribution density p(??|A,B) for given value Theta and fixed beta distribution parameters Aprior, Bprior.


```{r}
binomialLikelihood <- function(Theta, Size, mySample){
  prod(dbinom(mySample, p=Theta, size=1))
}
binomialPrior <- function(Theta, A, B) {
  dbeta(Theta, shape1 = A, shape2 = B)
}
```
Finally, write decision function metropolisRule<-function(thetaOld,thetaNew,mySample,funLikelihood,funPrior).
This function takes thetaOld, thetaNew and calculates pdecpdec.
Then depending on the value of pdecpdec the function returns value Rejection equal to TRUE if thetaNew is rejected and FALSE if not.
```{r} 
# Metropolis rule. 
# create decision value. product of likelihood * Prior of new state / likelihood*prior of old state.
metropolisRule<-function(thetaOld,
                         thetaNew,
                         mySample,
                         funLikelihood,
                         funPrior){
  decValue<-funLikelihood(Theta=thetaNew,Size=ExtraParameters["Size"],mySample=mySample)*
    funPrior(Theta=thetaNew,A=ExtraParameters["A"],B=ExtraParameters["B"])/
    funLikelihood(Theta=thetaOld,Size=ExtraParameters["Size"],mySample=mySample)/
    funPrior(Theta=thetaOld,A=ExtraParameters["A"],B=ExtraParameters["B"])
  decValue<-min(1,decValue)

  # if decValue =1, rejection is false. If not it is true
  Rejection<-ifelse(runif(1)<=decValue,FALSE,TRUE)
  Rejection
}
```

### 2.2.3 Initiate parameters

```{r}
# Initiate
chainMax<-500               # length of Markov chain
Chain<-numeric(0)           # empty vector for the chain
Rejected<-numeric(0)        # empty vector for rejected values
chainLength<-length(Chain)  # current length of chain
tuneSigma<-.1               # standard deviation of proposal distribution
priorParam<-c(A=10,B=10)    # parameters of prior beta distribution
ExtraParameters<-c(20,priorParam["A"],priorParam["B"])
names(ExtraParameters)<-c("Size","A","B")
ExtraParameters             # all extra global parameters

ThetaOld <- 0.5
```

### 2.2.4 Run MCMC

Run loop until number of accepted values in Chain reaches chainMax
In this loop:
  Generate new proposal ThetaNew
  Apply Metropolis decision rule. If new proposal is accepted add it to Chain and increase chainLength, then reset ThetaOld to  ThetaNew. If new proposal is rejected then add it to vector Rejected
  
```{r}
while (chainLength<chainMax) {
  ThetaNew <- proposalFun(ThetaOld)
  if (metropolisRule(ThetaOld, ThetaNew,Y, binomialLikelihood, binomialPrior)) {
    Rejected <- c(Rejected, ThetaNew)
  } else {
    Chain <<-c(Chain, ThetaNew)
    ThetaOld<<-ThetaNew
    chainLength<<-length(Chain)
  }
}
```

## 2.3 Checking results

After simulating Markov chain check the results.

###2.3.1 Analytical solution

Compare prior parameters and posterior parmas given by formulas
```{r}
priorParam
(posteriorParam <- priorParam+c(sum(Y), length(Y)-sum(Y)))
```
Compare mean values of prior and posterior distributions with MLE
```{r}
meanValues<-c(Prior=priorParam[1]/sum(priorParam),
              MLE=mean(Y),
              Posterior=posteriorParam[1]/sum(posteriorParam))
names(meanValues)<-c("Prior","MLE","Posterior")
meanValues
```
### 2.3.2 Histograms of accepted and rejected values
```{r}
hist(Chain)
hist(Rejected)
```
Histogram of rejected should have less values around the highest density value of theta (0.5) This is becuase thetas close to that level are rarely rejected, due to the probability of decision rule.

Compare distribution of the simulated Markov chain with the analytical posterior distribution.
```{r}
den<-density(Chain)
plot(den,col="orange")
lines(den$x,dbeta(den$x,posteriorParam[1],posteriorParam[2]),type="l",col="blue")
```
Compare prior and posterior densities
```{r}
X<-seq(from=.01,to=.99,by=.01)
plot(X,dbeta(X,priorParam[1],priorParam[2]),type="l",col="orange",ylim=c(0,5))
lines(X,dbeta(X,posteriorParam[1],posteriorParam[2]),type="l",col="blue")
```

# 3 Gaussian distribution, one sample: known variance, unknown mena

## 3.1 Data

Simulate singe sample from Gaussian distribution with unknown mean theta and known standard deviation sigma

```{r}
mu<-120  
si<-25
nSample<-200
set.seed(7036)
Y<-rnorm(nSample,mean=mu,sd=si) # create a sample of 200 using a norm dist of mu and si
Theoretical.data<-c(Mean=mu,Sd=si) # mean and sd
plot(Y) # plot the sample
```

## 3.2 FNP approach

Estimate the mean of the distribution, check histogram, test hypothesis H0: mu==120 against 2 sided alternative Ha: mu!=120
```{r}
meanMLE <- mean(Y)
hist(Y,freq = F)
```
```{r}
plot(density(Y))
```

To test H0, use z statistic (known sigma)
Then calculate p value for 2 sided alternative
```{r}
(zStat<-(meanMLE-Theoretical.data["Mean"])/(Theoretical.data["Sd"]/sqrt(nSample)))
(sSidedP<-pnorm(zStat,lower.tail = F)+pnorm(-zStat,lower.tail = T))
# distribution of the data estimated by FNP
(MLE.data<-c(Mean=meanMLE,Sd=Theoretical.data["Sd"]))
```
## 3.3 Bayesian approach by formula, weak prior

Set the model as

yi???N(??,??),
where ?? is unknown and ??=25
Set prior distribution for the mean value as low informative Gaussian with parameters M=100, ????=200, i.e.
??~N(M,????).
```{r}
# set the parameters of the prior distribution
priorParamWeak<-c(Mean=100,Sd=200)  
```

Obtain posterior distribution using formulas for conjugate Gaussian distribution:
```{r}
precData<-nSample/Theoretical.data["Sd"]^2
precPrior<-1/priorParamWeak["Sd"]^2
wPrior<-unname(precPrior/(precPrior+precData))
wData<-unname(precData/(precPrior+precData))
pMean<-unname(priorParamWeak["Mean"]*wPrior+MLE.data["Mean"]*wData)
precPosterior<-precPrior+precData
pSd<-unname(1/sqrt(precPosterior))
(posterParamWeak<-c(Mean=pMean,Sd=pSd))
```
Bayesian estimate is obtained as ??post=??post= 100 ×× 7.811889710^{-5} ++ 121.0935118 × 0.9999219, showing no compromize between likelihood and prior.

Plot theoretical (simulated), estimated by MLE and estimated by Bayesian analysis densities.

```{r}
Bayes.dataWeak<-c(posterParamWeak["Mean"],Theoretical.data["Sd"])
X<-seq(from=115,to=125,by=1)
plot(X,dnorm(X,Theoretical.data[1],Theoretical.data[2]),
     type="l",xlab="X",ylab="Density")
lines(X,dnorm(X,meanMLE,si),col="orange",lwd=3)
lines(X,dnorm(X,posterParamWeak[1],si),col="blue")
legend("topright",
        legend=c("Theoretical","MLE","Bayesian"),
        col=c("black","orange","blue"),
       lty=1)
```
For the sample length 200 and low informative prior Bayesian estimate is very close to MLE in comparison with the prior.
In addition standard deviation of the posterior distribution collapsed from 200 to 1.7676979

```{r}
rbind(Theoretical.data,MLE.data,Bayes.dataWeak)
rbind(priorParamWeak,posterParamWeak)
```

## 3.4 Bayesian approach by formula, strong prior

Repeat Bayesian analysis with stronger prior M=100, sigma =2
```{r}
#rbind(priorParamStrong,posterParamStrong)
```
Bayesian estimate of the mean shifted towards mode of the prior distribution: now the mean of posterior distribution is obtained by ??post=??post= 100 ×× 0.4385965 ++ 121.0935118 ×× 0.5614035.

Shift of the lower level parameter towards mode of the higher level parameter is shrinkage.

## 3.5 Using JAGS to esitmate mean of normal distribution with known variance
###3.5.1 Data
```{r}
dataPath="C:/Users/JohntheGreat/Documents/MSCA/BayesianMethods/Week4_MonteCarlo"
suppressWarnings(source(paste(dataPath,"DBDA2E-utilities.R",sep="/")))
```
Prepare the data list in format for JAGS


```{r}
y <- Y
dataList <- list(y=y, nSample=nSample)
names(dataList)
```

### 3.5.2 Preparation of the model for JAGS

Interpretation of the diagram starts from the bottom.
Each arrow of the diagram corresponds to a line of description code for the model.

Line 1 of the model describes generation of the data according to the likelihood function: yi???dnorm(??,??).
Line 2 of the model describes generation of the parameter ?? from the prior distribution: ?????dnorm(M,????).
In this case parameters of the prior distribution should be defined. For example, like above set M=100,????=200.
Data are described as for loop.
Prior is described as typical formula:
"lower order parameter ?????? density(higher order parameters)".

Note. In JAGS normal distribution is specified with mean value and precision, i.e. instead of dnorm(??,??) use dnorm(??,1??2)d.
```{r}
model1NormalSample=
"
model {
  for (i in 1:nSample){
    y[i]~dnorm(theta, 1/25^2)
  }
  theta~dnorm(100,1/200^2)
}
"
```
Note that variables names for the data and data length y, nSample in the description have to match the names of the data list.

The next step of preparation of the model description for JAGS is saving it to a temporary text file  Tempmodel.txt.
```{r}
writeLines(model1NormalSample,con="Tempmodel.txt")
```


### 3.5.3 Initializing Markov chains

To initialize trajectories define a list of lists with several values or create a function that returns an init value every time it is called.
JAGS will call this function when it starts a new chain.

General syntax of initiation function is:
```{r}
initsList<-function() {
  MLE <- mean(y)
  thetaInit <- rnorm(1, MLE, sd=10)
  return(list(theta=thetaInit))
}
initsList()
```
### 3.5.4 Sending information to JAGS

Next step is getting all information to JAGS using jags.model() from library rjags.
This function transfers the data, the model specification and the initial values to JAGS and requests JAGS to select appropriate sampling method.
```{r}
suppressWarnings(library(rjags))
jagsModel <- jags.model(file="TempModel.txt", data=dataList, n.chains=3, n.adapt = 500)
```

In jags.model, the parameter n.chains specifies the number of chains to run and the parameter n.adapt sets the number of steps JAGS can take to tune the sampling method(defaults to 1000)

The object returned by jags.model contains all info that we need to communicate to JAGS about the problem in format suitable for JAGS

### 3.5.5 Running MCMC in JAGS:burn in and main run

Now run JAGS chain for 600 steps to complete burn in
```{r}
update(jagsModel, n.iter=600)
```
After completeing burn in generate MCMC trajectories representing the posterior distribution for the model
```{r}
# MCMC trajectories represnet the posterior distribution for the model
codaSamples <- coda.samples(jagsModel, variable.names = c("theta"), n.iter=3334)
list.samplers(jagsModel)
# contains the chains
head(codaSamples)
```
Besides the model specification object coda.samples() takes  a vector of character strings corresponding to the names of parameters to record variable.names and the number of iterations to run n.iter
In this example ther are 3 chains to the total # of iterations will be about 10k, use functions list.samplers to show the samplers applied to the model

### 3.5.6 Analyzing convergeence

Analyze convergence of chains using the following tools

1. Summary
```{r}
summary(codaSamples)
```
2. traceplot
```{r}
# traceplot
coda::traceplot(codaSamples)
# density plot
densplot(codaSamples)
plot(codaSamples)
```
3. Autocorrelation and effective size
```{r}
autocorr.plot(codaSamples, ask=F)
```

```{r}
effectiveSize(codaSamples)
#The ESS number is consistent with the autocorrelation function
```

4. Shrink factor
```{r}
gelman.diag(codaSamples)
gelman.plot(codaSamples)
```

### 3.5.7 Analyzing and interpreting the results

1. Find the mean values and standard deviation of posterior distributions corresponding to differnt chains and compare them with MLE:
```{r}
chainmeans <- lapply(codaSamples, mean)
MLE <- sum(y)/nSample
chainSd <- lapply(codaSamples, sd)
rbind(Means=chainmeans, SD=chainSd)
```

Compare the posterior densities generated by 3 chains with analytical posterior distribution
```{r}
#set limits for histogram
(l<-min(unlist(codaSamples))-.05)
(h<-max(unlist(codaSamples))+.05)
histBreaks <- seq(l,h,by=0.05)
postHist <- lapply(codaSamples, hist, breaks=histBreaks)
```
```{r}
plot(postHist[[1]]$mids,postHist[[1]]$density,type="l",
     col="black",lwd=2,ylab="Distribution Density",xlab="Theta")
lines(postHist[[2]]$mids,postHist[[2]]$density,type="l",col="red",lwd=2)
lines(postHist[[3]]$mids,postHist[[3]]$density,type="l",col="blue",lwd=2)
lines(postHist[[3]]$mids,
      dnorm(postHist[[3]]$mids,posterParamWeak["Mean"],posterParamWeak["Sd"]),
      type="l",col="green",lwd=3)
legend("topright",legend=c("Chain1","Chain2","Chain3","Theoretical"),col=c("black","red","blue","green"),lwd=2)
```
```{r}
head(postHist[[1]]$equidist)
```

# 4. Hierarchical model: two groups of Gaussian observations

Consider now two groups of Gaussian observations with unkown means mu1, mu2 and same known standard deviation sigma =25



## 4.1 Data
Create data, prepare for JAGs
keep the first group sample mean 120, sd 25. simulate 2nd sample with mean 150 and sd 25. combine both samples together and add ssecond column containing group number
```{r}
nSample<-200
set.seed(7952)
Y2<-rnorm(nSample,mean=150,sd=25)
Y2<-rbind(cbind(Y,rep(1,nSample)),cbind(Y2,rep(2,nSample)))
colnames(Y2)<-c("y","s")
den1<-density(subset(Y2,Y2[,2]==1)[,1])
den2<-density(subset(Y2,Y2[,2]==2)[,1])
plot(den1$x,den1$y,type="l")
lines(den2$x,den2$y,col="blue")

y=Y2[,"y"]
s<-Y2[,"s"]
nSample<-length(y)
nGr<-length(unique(s))
dataList<-list(y=y,s=s,nSample=nSample,nGr=nGr)
names(dataList)
```
## 4.2 Different mean values, common weak prior

IN this section consider model structure with common weak prior for mean values of both groups. Think about situations in which common prior is a reasonable assumption
Select hyper-parameters of the common prior normal distribution as in the previous section: M=100, ????=200

preparethe model string

```{r}
model2NormalSamples=
"
  model {
    for(i in 1:nSample){
      y[i]~dnorm(theta[s[i]], 1/25^2)
    }
    for (sIdx in 1:nGr) {
      theta[sIdx]~dnorm(100,1/200^2)
    }
  }

"
writeLines(model2NormalSamples, con="TEMPmodel.txt")
```

### 4.2.2 Initialization and sending the model to JAGS

initialize chains randomly around MLE
```{r}
# create initializtion
  initsList = function() {
    thetaInit = rep(0,nGr)
    for ( sIdx in 1:nGr ) { # for each group
      includeRows = ( s == sIdx ) # identify rows of this group
      yThisGr = y[includeRows]  # extract data of this group
      initThisGr<-rnorm(1,mean=mean(yThisGr),sd=15) # random around MLE
      thetaInit[sIdx] = initThisGr 
    }
    return( list( theta=thetaInit ) )
  }
initsList()
# send the model to JAGS
parameters <- c("theta")
adaptSteps <-500 
burnInSteps <- 500
nChains <- 3
numSavedSteps <- 50000
nIter <- numSavedSteps/nChains

jagsModel= jags.model("TEMPmodel.txt", data=dataList, inits=initsList, n.chains=nChains, n.adapt = adaptSteps)
```
### 4.2.3 Running the model

Run burn in by using update()
```{r}
update(jagsModel, n.iter=burnInSteps)
# Make the main run
codaSamples2Groups1Prior <- coda.samples(model = jagsModel,variable.names = parameters, n.iter = nIter)
head(codaSamples2Groups1Prior)
list.samplers(jagsModel)
```

###4.2.4 Analysis

Analyze convergence
```{r}
summary(codaSamples2Groups1Prior)
# trace plot and density
plot(codaSamples2Groups1Prior)
# autocorrelation plots of chains
autocorr.plot(codaSamples2Groups1Prior,ask=F)
# effective size of chains
effectiveSize(codaSamples2Groups1Prior)
# gelman plot to check for convergence
gelman.diag(codaSamples2Groups1Prior)
gelman.plot(codaSamples2Groups1Prior)
# check estimated means
matrix(unlist(lapply(codaSamples2Groups1Prior,function(z) apply(z,2,mean))),ncol=3)
# plot posterior densities
plot(density(codaSamples2Groups1Prior[[1]][,1]),xlim=c(110,160),ylim=c(0,.25))
lines(density(codaSamples2Groups1Prior[[1]][,2]))
lines(density(codaSamples2Groups1Prior[[2]][,1]),col="orange")
lines(density(codaSamples2Groups1Prior[[2]][,2]),col="orange")
lines(density(codaSamples2Groups1Prior[[3]][,1]),col="blue")
lines(density(codaSamples2Groups1Prior[[3]][,2]),col="blue")
```
Calculate HDIs for each chain
```{r}
suppressWarnings(library(HDInterval))
lapply(codaSamples2Groups1Prior,function(z) hdi(as.matrix(z)))
# find differences between theta1 and theta2
chainDiffs<-lapply(codaSamples2Groups1Prior,function(z) z[,2]-z[,1])
lapply(chainDiffs,function(z) hdi(as.matrix(z)))
```
Find left 95% HDI boundaries for the chain differences and plot them
```{r}
  (leftBounds<-unlist(lapply(chainDiffs,function(z) hdi(as.matrix(z))[1])))
  plot(density(chainDiffs[[1]]),xlim=c(15,35),ylim=c(0,.2),col="black")
  lines(density(chainDiffs[[2]]),col="red")
  lines(density(chainDiffs[[3]]),col="blue")
  abline(v=leftBounds,col=c("black","orange","blue"))
```
For the given sample and prior distribution mean values of the 2 groups are clearly different based on 95% HDI for all 3 chains.

## 4.3 Experimenting with prior hyperparameters

Repeat calculations with different prior hyperparameters.
To do that organize all steps of running MCMC with JAGS in a function  runMCMC2Groups<-function(prMean,prSD,dat), where prMean and prSD are the the prior hyperparameters and dat is the data list.

Note that in order to pass arguments of the function to JAGS model description you need to append them to the data list:

First put them in a list hyper<-list(hypeMean=prMean,hypeSD=prSD)
Then combine list hyper with the data when run jags.model():
jags.model("TEMPmodel.txt",data=append(dat,hyper),...)
After that parameters can be used in the description of the model as  dnorm(hypeMean,1/hypeSD^2).
The function should return the simulated chains.

```{r}
runMCMC2groups <- function(prMean, prSD,dat){
  # Create model description
  model2NormalSamples=
"
  model {
    for (i in 1:nSample) {
      y[i]~dnorm(theta[s[i]], 1/25^2)
    }
  for (sIdx in 1:nGr) {
# different thetas from the same prior
    theta[sIdx} ~ dnorm(hypeMean, 1/hypeSD^2)   
  }  
}  
"
# save model description for JAGS  
writeLines(model2NormalSamples, con="TEMPmodel.txt")
# save parameter values
parameters=c("theta")
adaptSteps=500
burnInSteps=500
nChains=3
numSavedSteps <- 50000
nIter=ceiling(numSavedSteps/nChains)
hyper <- list(hypeMean=prMean, hypeSD=prSD)
# create jags model
jagsModel=jags.model("TEMPmodel.txt", data=append(dat,hyper), inits=initsList, n.chains=nChains, n.adapt=adaptSteps)
# run burn in steps
update(jagsModel, n.iter=burnInSteps)
# run MCMC
codaSamplesResult <- coda.samples(jagsModel, variable.names=parameters, n.iter=nIter)
# return result
codaSamplesResult

  
  
}
```
```{r}
runMCMC2Groups<-function(prMean,prSD,dat){
  model2NormalSamples = 
"
model {
    for ( i in 1:nSample ) {
      y[i] ~ dnorm( theta[s[i]],1/25^2 )
    }
    for ( sIdx in 1:nGr ) {      # Different thetas from same prior
      theta[sIdx] ~ dnorm(hypeMean,1/hypeSD^2) 
    }
}
" # close quote for modelString
writeLines( model2NormalSamples , con="TEMPmodel.txt" )
parameters = c("theta")      # The parameters to be monitored
adaptSteps = 500             # Number of steps to adapt the samplers
burnInSteps = 500            # Number of steps to burn-in the chains
nChains = 3                  # nChains should be 2 or more for diagnostics 
numSavedSteps<-50000
nIter = ceiling(numSavedSteps / nChains )
hyper<-list(hypeMean=prMean,hypeSD=prSD)
# Create, initialize, and adapt the model:
jagsModel = jags.model( "TEMPmodel.txt" , data=append(dat,hyper) , inits=initsList , 
                          n.chains=nChains , n.adapt=adaptSteps )
update( jagsModel , n.iter=burnInSteps )
codaSamplesResult<- coda.samples( jagsModel,variable.names=parameters,n.iter=nIter)
codaSamplesResult
}
```

## 4.3.1 Run 130/200
```{r}
Run.130.200 <- runMCMC2groups(130,200,dataList)
lapply(Run.130.200,function(z) apply(z,2,mean))
chainDiffs<-lapply(Run.130.200,function(z) z[,2]-z[,1])
lapply(chainDiffs,function(z) hdi(as.matrix(z)))
```

### 4.3.2 130/0.2
```{r}
Run.130.2<-runMCMC2Groups(130,.2,dataList)
lapply(Run.130.2,function(z) apply(z,2,mean))
chainDiffs<-lapply(Run.130.2,function(z) z[,2]-z[,1])
lapply(chainDiffs,function(z) hdi(as.matrix(z)))
```

### 4.3.3 100/0.2
```{r}
Run.100.2<-runMCMC2Groups(100,.2,dataList)
lapply(Run.100.2,function(z) apply(z,2,mean))
chainDiffs<-lapply(Run.100.2,function(z) z[,2]-z[,1])
lapply(chainDiffs,function(z) hdi(as.matrix(z)))
```

###4.3.4 170/0.2

```{r}
Run.170.2<-runMCMC2Groups(170,.2,dataList)
lapply(Run.170.2,function(z) apply(z,2,mean))
chainDiffs<-lapply(Run.170.2,function(z) z[,2]-z[,1])
lapply(chainDiffs,function(z) hdi(as.matrix(z)))
```

##  4.4 FNP approach: ANOVA

```{r}
head(Y2)
mANOVA<-lm(y~as.factor(s),as.data.frame(Y2))
summary(mANOVA)
anova(mANOVA)
```



